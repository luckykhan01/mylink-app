import os
import uuid
import json
import pickle
from typing import List, Optional, Dict, Any, Tuple
from datetime import datetime
from pathlib import Path

import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field, field_validator
from openai import OpenAI
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

if not OPENAI_API_KEY:
    raise ValueError("OPENAI_API_KEY environment variable is required")

client = OpenAI(api_key=OPENAI_API_KEY)
print("ü§ñ AI Assistant running with OpenAI API")
app = FastAPI(title="AI Assistant (CV‚ÜîVacancy)", version="0.1.0")

# –°–∏—Å—Ç–µ–º–∞ embeddings –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞
EMBEDDINGS_CACHE_FILE = Path("embeddings_cache.pkl")
SIMILARITY_THRESHOLD = 0.8  # –ü–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏ –¥–ª—è —Å–∏–Ω–æ–Ω–∏–º–æ–≤

class EmbeddingsManager:
    def __init__(self):
        self.cache = self._load_cache()
        self.embedding_model = "text-embedding-3-small"
    
    def _load_cache(self) -> Dict[str, np.ndarray]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫—ç—à embeddings"""
        if EMBEDDINGS_CACHE_FILE.exists():
            try:
                with open(EMBEDDINGS_CACHE_FILE, 'rb') as f:
                    return pickle.load(f)
            except:
                return {}
        return {}
    
    def _save_cache(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∫—ç—à embeddings"""
        with open(EMBEDDINGS_CACHE_FILE, 'wb') as f:
            pickle.dump(self.cache, f)
    
    def get_embedding(self, text: str) -> np.ndarray:
        """–ü–æ–ª—É—á–∞–µ—Ç embedding –¥–ª—è —Ç–µ–∫—Å—Ç–∞"""
        if text in self.cache:
            return self.cache[text]
        
        try:
            response = client.embeddings.create(
                model=self.embedding_model,
                input=text
            )
            embedding = np.array(response.data[0].embedding)
            self.cache[text] = embedding
            self._save_cache()
            return embedding
        except Exception as e:
            print(f"Error getting embedding for '{text}': {e}")
            return np.zeros(1536)  # –†–∞–∑–º–µ—Ä embedding –¥–ª—è text-embedding-3-small
    
    def are_similar(self, text1: str, text2: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è—é—Ç—Å—è –ª–∏ –¥–≤–∞ —Ç–µ–∫—Å—Ç–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –ø–æ—Ö–æ–∂–∏–º–∏"""
        if not text1 or not text2:
            return False
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        text1 = text1.strip().lower()
        text2 = text2.strip().lower()
        
        # –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
        if text1 == text2:
            return True
        
        # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ
        emb1 = self.get_embedding(text1)
        emb2 = self.get_embedding(text2)
        
        similarity = cosine_similarity([emb1], [emb2])[0][0]
        return similarity >= SIMILARITY_THRESHOLD
    
    def find_best_match(self, target: str, candidates: List[str]) -> Optional[str]:
        """–ù–∞—Ö–æ–¥–∏—Ç –ª—É—á—à–∏–π —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –º–∞—Ç—á —Å—Ä–µ–¥–∏ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤"""
        if not target or not candidates:
            return None
        
        target_emb = self.get_embedding(target.strip().lower())
        best_match = None
        best_similarity = 0
        
        for candidate in candidates:
            if not candidate:
                continue
                
            candidate_emb = self.get_embedding(candidate.strip().lower())
            similarity = cosine_similarity([target_emb], [candidate_emb])[0][0]
            
            if similarity > best_similarity and similarity >= SIMILARITY_THRESHOLD:
                best_similarity = similarity
                best_match = candidate
        
        return best_match

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä embeddings
embeddings_manager = EmbeddingsManager()

# --- CORS –¥–ª—è —Ñ—Ä–æ–Ω—Ç–∞/–±—ç–∫–∞ (–¥–æ–±–∞–≤—å —Å—é–¥–∞ —Å–≤–æ–∏ –¥–æ–º–µ–Ω—ã) ---
app.add_middleware(
    CORSMiddleware,
    allow_origins=os.getenv("CORS_ALLOW_ORIGINS", "*").split(","),
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

SESSIONS: Dict[str, Dict[str, Any]] = {}

class Candidate(BaseModel):
    full_name: Optional[str] = None
    city: Optional[str] = None
    experience_years: Optional[float] = Field(None, ge=0)
    position: Optional[str] = None
    education: Optional[str] = None
    languages: List[str] = Field(default_factory=list)
    salary_expectation: Optional[int] = Field(None, ge=0)
    employment_type: Optional[str] = None
    skills: List[str] = Field(default_factory=list)
    seniority: Optional[str] = None
    source_text: Optional[str] = None

class Vacancy(BaseModel):
    title: Optional[str] = None
    city: Optional[str] = None
    min_experience_years: Optional[float] = Field(None, ge=0)
    required_position: Optional[str] = None
    education_required: Optional[str] = None
    languages_required: List[str] = Field(default_factory=list)
    salary_min: Optional[int] = Field(None, ge=0)
    salary_max: Optional[int] = Field(None, ge=0)
    employment_type: Optional[str] = None
    must_have_skills: List[str] = Field(default_factory=list)
    nice_to_have_skills: List[str] = Field(default_factory=list)
    seniority: Optional[str] = None
    source_text: Optional[str] = None

    @field_validator("salary_max")
    @classmethod
    def check_salary(cls, v, info):
        if v is not None and info.data.get("salary_min") is not None and v < info.data["salary_min"]:
            raise ValueError("salary_max must be >= salary_min")
        return v

class ParseRequest(BaseModel):
    text: str
    kind: str  # "cv" | "vacancy"

class AnalyzeRequest(BaseModel):
    cv: Optional[Candidate] = None
    vacancy: Optional[Vacancy] = None
    cv_text: Optional[str] = None
    vacancy_text: Optional[str] = None
    session_id: Optional[str] = None

class AnalyzeResult(BaseModel):
    session_id: str
    relevance_percent: int
    reasons: List[str]
    mismatches: Dict[str, Dict[str, Any]]
    followup_questions: List[str]
    candidate: Candidate
    vacancy: Vacancy
    summary_for_employer: str

class ChatTurn(BaseModel):
    session_id: str
    message_from_candidate: str

class ChatResult(BaseModel):
    session_id: str
    bot_replies: List[str]
    relevance_percent: int
    reasons: List[str]
    summary_for_employer: str

def _structured_extract(schema_name: str, schema: Dict[str, Any], prompt: str) -> Dict[str, Any]:
    # –£–ª—É—á—à–µ–Ω–Ω—ã–π —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
    system_prompt = f"""–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É —Ä–µ–∑—é–º–µ –∏ –≤–∞–∫–∞–Ω—Å–∏–π. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ - —Ç–æ—á–Ω–æ –∏–∑–≤–ª–µ—á—å —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ —Ç–µ–∫—Å—Ç–∞.

–í–ê–ñ–ù–´–ï –ü–†–ò–ù–¶–ò–ü–´:
1. –ë—É–¥—å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Ç–æ—á–Ω—ã–º –∏ –≤–Ω–∏–º–∞—Ç–µ–ª—å–Ω—ã–º –∫ –¥–µ—Ç–∞–ª—è–º
2. –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–π null –∏–ª–∏ –ø—É—Å—Ç—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
3. –î–ª—è –º–∞—Å—Å–∏–≤–æ–≤ (–Ω–∞–≤—ã–∫–∏, —è–∑—ã–∫–∏) –∏–∑–≤–ª–µ–∫–∞–π –≤—Å–µ —É–ø–æ–º—è–Ω—É—Ç—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
4. –î–ª—è —á–∏—Å–µ–ª (–æ–ø—ã—Ç, –∑–∞—Ä–ø–ª–∞—Ç–∞) –∏–∑–≤–ª–µ–∫–∞–π —Ç–æ—á–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
5. –ù–æ—Ä–º–∞–ª–∏–∑—É–π –¥–∞–Ω–Ω—ã–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, "full-time" ‚Üí "—Ñ—É–ª–ª—Ç–∞–π–º")
6. –£—á–∏—Ç—ã–≤–∞–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ —Å–∏–Ω–æ–Ω–∏–º—ã

–ü–†–ê–í–ò–õ–ê –ò–ó–í–õ–ï–ß–ï–ù–ò–Ø:
- –û–ø—ã—Ç —Ä–∞–±–æ—Ç—ã: –∏–∑–≤–ª–µ–∫–∞–π –≤ –≥–æ–¥–∞—Ö (float), –Ω–∞–ø—Ä–∏–º–µ—Ä 2.5 –ª–µ—Ç
- –ó–∞—Ä–ø–ª–∞—Ç–∞: –∏–∑–≤–ª–µ–∫–∞–π –≤ —á–∏—Å–ª–æ–≤–æ–º —Ñ–æ—Ä–º–∞—Ç–µ –±–µ–∑ –≤–∞–ª—é—Ç—ã
- –ù–∞–≤—ã–∫–∏: –∏–∑–≤–ª–µ–∫–∞–π –≤—Å–µ —É–ø–æ–º—è–Ω—É—Ç—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã
- –Ø–∑—ã–∫–∏: –∏–∑–≤–ª–µ–∫–∞–π –≤—Å–µ —É–ø–æ–º—è–Ω—É—Ç—ã–µ —è–∑—ã–∫–∏ —Å —É—Ä–æ–≤–Ω–µ–º (–µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω)
- –¢–∏–ø –∑–∞–Ω—è—Ç–æ—Å—Ç–∏: –Ω–æ—Ä–º–∞–ª–∏–∑—É–π –∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º —Ñ–æ—Ä–º–∞–º (—Ñ—É–ª–ª—Ç–∞–π–º, –ø–∞—Ä—Ç—Ç–∞–π–º, —Ñ—Ä–∏–ª–∞–Ω—Å, –∫–æ–Ω—Ç—Ä–∞–∫—Ç)

–í–µ—Ä–Ω–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å—Ç—Ä–æ–≥–æ –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON —Å–æ–≥–ª–∞—Å–Ω–æ —Å—Ö–µ–º–µ."""

    resp = client.chat.completions.create(
        model=OPENAI_MODEL,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": prompt}
        ],
        response_format={"type": "json_schema", "json_schema": {"name": schema_name, "schema": schema}},
        temperature=0.1,  # –ù–∏–∑–∫–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
    )
    import json
    return json.loads(resp.choices[0].message.content)

def extract_candidate(text: str) -> Candidate:
    schema = {
        "type": "object",
        "properties": {
            "full_name": {"type": "string"},
            "city": {"type": "string"},
            "experience_years": {"type": "number"},
            "position": {"type": "string"},
            "education": {"type": "string"},
            "languages": {"type": "array", "items": {"type": "string"}},
            "salary_expectation": {"type": "integer"},
            "employment_type": {"type": "string"},
            "skills": {"type": "array", "items": {"type": "string"}},
            "seniority": {"type": "string"}
        },
        "required": [],
        "additionalProperties": False
    }
    prompt = f"""–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ä–µ–∑—é–º–µ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞ –∏ –∏–∑–≤–ª–µ–∫–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é.

–¢–ï–ö–°–¢ –†–ï–ó–Æ–ú–ï:
{text}

–ò–ù–°–¢–†–£–ö–¶–ò–ò –ü–û –ò–ó–í–õ–ï–ß–ï–ù–ò–Æ:
1. –ü–æ–ª–Ω–æ–µ –∏–º—è: –∏–∑–≤–ª–µ–∫–∏ –§–ò–û –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–æ
2. –ì–æ—Ä–æ–¥: –∏–∑–≤–ª–µ–∫–∏ –≥–æ—Ä–æ–¥ –ø—Ä–æ–∂–∏–≤–∞–Ω–∏—è/—Ä–∞–±–æ—Ç—ã
3. –û–ø—ã—Ç —Ä–∞–±–æ—Ç—ã: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª–µ—Ç –æ–ø—ã—Ç–∞ (float, –Ω–∞–ø—Ä–∏–º–µ—Ä 2.5)
4. –ü–æ–∑–∏—Ü–∏—è: –¥–æ–ª–∂–Ω–æ—Å—Ç—å/—Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ—Å—Ç—å
5. –û–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ: —É—Ä–æ–≤–µ–Ω—å –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è, –≤—É–∑, —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ—Å—Ç—å
6. –Ø–∑—ã–∫–∏: –≤—Å–µ —É–ø–æ–º—è–Ω—É—Ç—ã–µ —è–∑—ã–∫–∏ —Å —É—Ä–æ–≤–Ω–µ–º (–Ω–∞–ø—Ä–∏–º–µ—Ä, "–∞–Ω–≥–ª–∏–π—Å–∫–∏–π B2")
7. –ó–∞—Ä–ø–ª–∞—Ç–∞: –æ–∂–∏–¥–∞–µ–º–∞—è –∑–∞—Ä–ø–ª–∞—Ç–∞ –≤ —á–∏—Å–ª–æ–≤–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
8. –¢–∏–ø –∑–∞–Ω—è—Ç–æ—Å—Ç–∏: —Ñ—É–ª–ª—Ç–∞–π–º/–ø–∞—Ä—Ç—Ç–∞–π–º/—Ñ—Ä–∏–ª–∞–Ω—Å/–∫–æ–Ω—Ç—Ä–∞–∫—Ç/—Å—Ç–∞–∂–∏—Ä–æ–≤–∫–∞
9. –ù–∞–≤—ã–∫–∏: –≤—Å–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏, –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã, —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∏
10. –£—Ä–æ–≤–µ–Ω—å: junior/middle/senior/lead

–í–ê–ñ–ù–û: –î–ª—è –º–∞—Å—Å–∏–≤–æ–≤ (languages, skills) –≤—Å–µ–≥–¥–∞ –≤–æ–∑–≤—Ä–∞—â–∞–π –ø—É—Å—Ç–æ–π –º–∞—Å—Å–∏–≤ [] –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç, –ù–ï –≤–æ–∑–≤—Ä–∞—â–∞–π null."""
    data = _structured_extract("candidate_schema", schema, prompt)
    
    # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º None –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –º–∞—Å—Å–∏–≤–æ–≤
    if data.get("languages") is None:
        data["languages"] = []
    if data.get("skills") is None:
        data["skills"] = []
    
    return Candidate(**data, source_text=text)

def extract_vacancy(text: str) -> Vacancy:
    schema = {
        "type": "object",
        "properties": {
            "title": {"type": "string"},
            "city": {"type": "string"},
            "min_experience_years": {"type": "number"},
            "required_position": {"type": "string"},
            "education_required": {"type": "string"},
            "languages_required": {"type": "array", "items": {"type": "string"}},
            "salary_min": {"type": "integer"},
            "salary_max": {"type": "integer"},
            "employment_type": {"type": "string"},
            "must_have_skills": {"type": "array", "items": {"type": "string"}},
            "nice_to_have_skills": {"type": "array", "items": {"type": "string"}},
            "seniority": {"type": "string"}
        },
        "required": [],
        "additionalProperties": False
    }
    prompt = f"""–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –æ–ø–∏—Å–∞–Ω–∏–µ –≤–∞–∫–∞–Ω—Å–∏–∏ –∏ –∏–∑–≤–ª–µ–∫–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é.

–¢–ï–ö–°–¢ –í–ê–ö–ê–ù–°–ò–ò:
{text}

–ò–ù–°–¢–†–£–ö–¶–ò–ò –ü–û –ò–ó–í–õ–ï–ß–ï–ù–ò–Æ:
1. –ù–∞–∑–≤–∞–Ω–∏–µ: –∑–∞–≥–æ–ª–æ–≤–æ–∫ –≤–∞–∫–∞–Ω—Å–∏–∏
2. –ì–æ—Ä–æ–¥: –≥–æ—Ä–æ–¥ —Ä–∞–±–æ—Ç—ã
3. –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –æ–ø—ã—Ç: –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –æ–ø—ã—Ç—É –≤ –≥–æ–¥–∞—Ö (float)
4. –¢—Ä–µ–±—É–µ–º–∞—è –ø–æ–∑–∏—Ü–∏—è: –¥–æ–ª–∂–Ω–æ—Å—Ç—å/—Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ—Å—Ç—å
5. –û–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ: —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—é
6. –Ø–∑—ã–∫–∏: —Ç—Ä–µ–±—É–µ–º—ã–µ —è–∑—ã–∫–∏ —Å —É—Ä–æ–≤–Ω–µ–º
7. –ó–∞—Ä–ø–ª–∞—Ç–∞: –¥–∏–∞–ø–∞–∑–æ–Ω –∑–∞—Ä–ø–ª–∞—Ç—ã (min –∏ max –≤ —á–∏—Å–ª–æ–≤–æ–º —Ñ–æ—Ä–º–∞—Ç–µ)
8. –¢–∏–ø –∑–∞–Ω—è—Ç–æ—Å—Ç–∏: —Ñ—É–ª–ª—Ç–∞–π–º/–ø–∞—Ä—Ç—Ç–∞–π–º/—Ñ—Ä–∏–ª–∞–Ω—Å/–∫–æ–Ω—Ç—Ä–∞–∫—Ç/—Å—Ç–∞–∂–∏—Ä–æ–≤–∫–∞
9. –û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –Ω–∞–≤—ã–∫–∏: –∫–ª—é—á–µ–≤—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã
10. –ñ–µ–ª–∞—Ç–µ–ª—å–Ω—ã–µ –Ω–∞–≤—ã–∫–∏: –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –Ω–∞–≤—ã–∫–∏ (nice to have)
11. –£—Ä–æ–≤–µ–Ω—å: junior/middle/senior/lead

–í–ê–ñ–ù–û: –î–ª—è –º–∞—Å—Å–∏–≤–æ–≤ (languages_required, must_have_skills, nice_to_have_skills) –≤—Å–µ–≥–¥–∞ –≤–æ–∑–≤—Ä–∞—â–∞–π –ø—É—Å—Ç–æ–π –º–∞—Å—Å–∏–≤ [] –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç, –ù–ï –≤–æ–∑–≤—Ä–∞—â–∞–π null."""
    data = _structured_extract("vacancy_schema", schema, prompt)
    
    # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º None –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –º–∞—Å—Å–∏–≤–æ–≤
    if data.get("languages_required") is None:
        data["languages_required"] = []
    if data.get("must_have_skills") is None:
        data["must_have_skills"] = []
    if data.get("nice_to_have_skills") is None:
        data["nice_to_have_skills"] = []
    
    return Vacancy(**data, source_text=text)

def _semantic_normalize(text: str) -> str:
    """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º embeddings"""
    if not text:
        return ""
    
    # –ë–∞–∑–æ–≤–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
    normalized = text.strip().lower()
    
    # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ —Ñ–æ—Ä–º—ã –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
    standard_forms = {
        # –¢–∏–ø—ã –∑–∞–Ω—è—Ç–æ—Å—Ç–∏
        "employment": {
            "—Ñ—É–ª–ª—Ç–∞–π–º": ["full-time", "full time", "–ø–æ–ª–Ω–∞—è –∑–∞–Ω—è—Ç–æ—Å—Ç—å", "–ø–æ–ª–Ω—ã–π —Ä–∞–±–æ—á–∏–π –¥–µ–Ω—å", "fulltime"],
            "–ø–∞—Ä—Ç—Ç–∞–π–º": ["part-time", "part time", "—á–∞—Å—Ç–∏—á–Ω–∞—è –∑–∞–Ω—è—Ç–æ—Å—Ç—å", "–Ω–µ–ø–æ–ª–Ω—ã–π —Ä–∞–±–æ—á–∏–π –¥–µ–Ω—å", "parttime"],
            "–∫–æ–Ω—Ç—Ä–∞–∫—Ç": ["contract", "–¥–æ–≥–æ–≤–æ—Ä", "–∫–æ–Ω—Ç—Ä–∞–∫—Ç–Ω–∞—è —Ä–∞–±–æ—Ç–∞"],
            "—Ñ—Ä–∏–ª–∞–Ω—Å": ["freelance", "—É–¥–∞–ª–µ–Ω–Ω–∞—è —Ä–∞–±–æ—Ç–∞", "remote", "—É–¥–∞–ª–µ–Ω–∫–∞", "—É–¥–∞–ª–µ–Ω–Ω–æ"],
            "—Å—Ç–∞–∂–∏—Ä–æ–≤–∫–∞": ["internship", "—Å—Ç–∞–∂–µ—Ä", "—Å—Ç–∞–∂–∏—Ä–æ–≤–∫–∞", "intern"]
        },
        # –ü–æ–∑–∏—Ü–∏–∏
        "position": {
            "backend": ["back-end", "–±—ç–∫–µ–Ω–¥", "–±—ç–∫-—ç–Ω–¥", "backend developer", "backend —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫"],
            "frontend": ["front-end", "—Ñ—Ä–æ–Ω—Ç–µ–Ω–¥", "—Ñ—Ä–æ–Ω—Ç-—ç–Ω–¥", "frontend developer", "frontend —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫"],
            "fullstack": ["full-stack", "—Ñ—É–ª–ª—Å—Ç–µ–∫", "full stack", "fullstack developer"],
            "devops": ["dev ops", "dev-ops", "—Å–∏—Å—Ç–µ–º–Ω—ã–π –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä", "—Å–∏—Å–∞–¥–º–∏–Ω"],
            "data scientist": ["data science", "–¥–∞—Ç–∞ —Å–∞–µ–Ω—Ç–∏—Å—Ç", "–∞–Ω–∞–ª–∏—Ç–∏–∫ –¥–∞–Ω–Ω—ã—Ö"],
            "mobile": ["–º–æ–±–∏–ª—å–Ω–∞—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞", "mobile developer", "ios", "android"],
            "qa": ["quality assurance", "—Ç–µ—Å—Ç–∏—Ä–æ–≤—â–∏–∫", "qa engineer", "—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ"]
        }
    }
    
    # –ü–æ–ø—ã—Ç–∫–∞ –Ω–∞–π—Ç–∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –º–∞—Ç—á
    for category, forms in standard_forms.items():
        for standard_form, variants in forms.items():
            if embeddings_manager.are_similar(normalized, standard_form):
                return standard_form
            for variant in variants:
                if embeddings_manager.are_similar(normalized, variant):
                    return standard_form
    
    return normalized

def _norm(s: Optional[str]) -> str:
    """–ü—Ä–æ—Å—Ç–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏"""
    return (s or "").strip().lower()

def score_and_mismatches(c: Candidate, v: Vacancy) -> Tuple[int, Dict[str, Dict[str, Any]], List[str]]:
    weights = {"city":0.15,"experience":0.2,"position":0.15,"education":0.1,"languages":0.15,"salary":0.1,"employment_type":0.05,"skills":0.1}
    score = 0.0; mismatches: Dict[str, Dict[str, Any]] = {}; reasons: List[str] = []

    if v.city and c.city:
        if embeddings_manager.are_similar(c.city, v.city): score+=weights["city"]
        else:
            mismatches["city"]={"expected":v.city,"actual":c.city}
            reasons.append(f"–ì–æ—Ä–æ–¥ –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è: –≤–∞–∫–∞–Ω—Å–∏—è ‚Äî {v.city}, –∫–∞–Ω–¥–∏–¥–∞—Ç ‚Äî {c.city}.")

    if v.min_experience_years is not None and c.experience_years is not None:
        if c.experience_years >= v.min_experience_years: score+=weights["experience"]
        else:
            mismatches["experience"]={"expected_min_years":v.min_experience_years,"actual_years":c.experience_years}
            reasons.append(f"–û–ø—ã—Ç –º–µ–Ω—å—à–µ —Ç—Ä–µ–±—É–µ–º–æ–≥–æ: –Ω—É–∂–Ω–æ –æ—Ç {v.min_experience_years} –ª–µ—Ç, —É –∫–∞–Ω–¥–∏–¥–∞—Ç–∞ {c.experience_years}.")

    if v.required_position and c.position:
        if embeddings_manager.are_similar(v.required_position, c.position): score+=weights["position"]
        else:
            mismatches["position"]={"expected":v.required_position,"actual":c.position}
            reasons.append(f"–î–æ–ª–∂–Ω–æ—Å—Ç—å –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è: —Ç—Ä–µ–±—É–µ—Ç—Å—è {v.required_position}, —É–∫–∞–∑–∞–Ω–æ {c.position}.")

    if v.education_required and c.education:
        if _norm(v.education_required) in _norm(c.education): score+=weights["education"]
        else:
            mismatches["education"]={"expected":v.education_required,"actual":c.education}
            reasons.append(f"–û–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç: —Ç—Ä–µ–±—É–µ—Ç—Å—è {v.education_required}, —É –∫–∞–Ω–¥–∏–¥–∞—Ç–∞ {c.education}.")

    if v.languages_required:
        cand = {_norm(x) for x in c.languages}
        req = {_norm(x) for x in v.languages_required}
        if req.issubset(cand): score+=weights["languages"]
        else:
            missing = list(req - cand)
            mismatches["languages"]={"missing":missing,"candidate":list(cand)}
            reasons.append(f"–ù–µ —Ö–≤–∞—Ç–∞–µ—Ç —è–∑—ã–∫–æ–≤: {', '.join(missing)}.")

    if c.salary_expectation is not None and (v.salary_min is not None or v.salary_max is not None):
        fits=True
        if v.salary_max is not None and c.salary_expectation>v.salary_max: fits=False
        if v.salary_min is not None and c.salary_expectation<v.salary_min: fits=False
        if fits: score+=weights["salary"]
        else:
            mismatches["salary"]={"vacancy_range":[v.salary_min,v.salary_max],"candidate":c.salary_expectation}
            reasons.append("–û–∂–∏–¥–∞–Ω–∏—è –ø–æ –∑–∞—Ä–ø–ª–∞—Ç–µ –≤—ã—Ö–æ–¥—è—Ç –∑–∞ –ø—Ä–µ–¥–µ–ª—ã –¥–∏–∞–ø–∞–∑–æ–Ω–∞ –≤–∞–∫–∞–Ω—Å–∏–∏.")

    if v.employment_type and c.employment_type:
        if embeddings_manager.are_similar(v.employment_type, c.employment_type): score+=weights["employment_type"]
        else:
            mismatches["employment_type"]={"expected":v.employment_type,"actual":c.employment_type}
            reasons.append(f"–§–æ—Ä–º–∞—Ç –∑–∞–Ω—è—Ç–æ—Å—Ç–∏: —Ç—Ä–µ–±—É–µ—Ç—Å—è {v.employment_type}, —É –∫–∞–Ω–¥–∏–¥–∞—Ç–∞ {c.employment_type}.")

    if v.must_have_skills:
        missing_skills = []
        for required_skill in v.must_have_skills:
            # –ò—â–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –º–∞—Ç—á —Å—Ä–µ–¥–∏ –Ω–∞–≤—ã–∫–æ–≤ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞
            found_match = False
            for candidate_skill in c.skills:
                if embeddings_manager.are_similar(required_skill, candidate_skill):
                    found_match = True
                    break
            
            if not found_match:
                missing_skills.append(required_skill)
        
        if not missing_skills: score+=weights["skills"]
        else:
            mismatches["skills"]={"missing":missing_skills}
            reasons.append(f"–ù–µ —Ö–≤–∞—Ç–∞–µ—Ç –∫–ª—é—á–µ–≤—ã—Ö –Ω–∞–≤—ã–∫–æ–≤: {', '.join(missing_skills)}.")

    return int(round(score*100)), mismatches, reasons

def make_followups(mismatches: Dict[str, Dict[str, Any]]) -> List[str]:
    context = "\n".join([f"{k}: {v}" for k,v in mismatches.items()]) if mismatches else "–Ω–µ—Ç —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–π"
    
    system_prompt = """–¢—ã ‚Äî –æ–ø—ã—Ç–Ω—ã–π HR-—Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç –∏ —Ä–µ–∫—Ä—É—Ç–µ—Ä. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ - —Å—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å —É—Ç–æ—á–Ω—è—é—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã –∫–∞–Ω–¥–∏–¥–∞—Ç—É.

–ü–†–ò–ù–¶–ò–ü–´ –†–ê–ë–û–¢–´:
1. –ë—É–¥—å –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–º –∏ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–º
2. –ó–∞–¥–∞–≤–∞–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã –ø–æ —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏—è–º
3. –ù–µ –¥–∞–≤–∏ –Ω–∞ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞, –∞ –ø–æ–º–æ–≥–∞–π —Ä–∞—Å–∫—Ä—ã—Ç—å –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª
4. –£—á–∏—Ç—ã–≤–∞–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Ä–∞–∑–≤–∏—Ç–∏—è
5. –ú–∞–∫—Å–∏–º—É–º 5 –≤–æ–ø—Ä–æ—Å–æ–≤, –ø–æ –æ–¥–Ω–æ–º—É –Ω–∞ –∫–∞–∂–¥–æ–µ —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–µ

–°–¢–ò–õ–¨ –í–û–ü–†–û–°–û–í:
- –ö–æ–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω—ã–µ –∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—â–∏–µ
- –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–µ –Ω–∞ –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Å–∏—Ç—É–∞—Ü–∏–∏
- –î–∞—é—â–∏–µ –∫–∞–Ω–¥–∏–¥–∞—Ç—É –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –æ–±—ä—è—Å–Ω–∏—Ç—å
- –ü–æ–º–æ–≥–∞—é—â–∏–µ –æ—Ü–µ–Ω–∏—Ç—å –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª —Ä–∞–∑–≤–∏—Ç–∏—è"""

    user_msg = f"""–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏—è –º–µ–∂–¥—É —Ä–µ–∑—é–º–µ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞ –∏ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º–∏ –≤–∞–∫–∞–Ω—Å–∏–∏ –∏ —Å—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π —É—Ç–æ—á–Ω—è—é—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã.

–†–ê–°–•–û–ñ–î–ï–ù–ò–Ø:
{context}

–°—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π 3-5 —É—Ç–æ—á–Ω—è—é—â–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–º–æ–≥—É—Ç:
1. –ü–æ–Ω—è—Ç—å –ø—Ä–∏—á–∏–Ω—ã —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–π
2. –û—Ü–µ–Ω–∏—Ç—å –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –∫–∞–Ω–¥–∏–¥–∞—Ç–∞
3. –í—ã—è—Å–Ω–∏—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Ä–∞–∑–≤–∏—Ç–∏—è
4. –ü—Ä–∏–Ω—è—Ç—å –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ

–í–µ—Ä–Ω–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON: {{"questions": ["–≤–æ–ø—Ä–æ—Å1", "–≤–æ–ø—Ä–æ—Å2", ...]}}"""

    schema = {"type":"object","properties":{"questions":{"type":"array","items":{"type":"string"}}},"required":["questions"],"additionalProperties":False}
    import json
    resp = client.chat.completions.create(
        model=OPENAI_MODEL,
        messages=[{"role":"system","content":system_prompt},{"role":"user","content":user_msg}],
        response_format={"type":"json_schema","json_schema":{"name":"followups","schema":schema}},
        temperature=0.3
    )
    data = json.loads(resp.choices[0].message.content)
    return data.get("questions", [])[:5]

def employer_summary(c: Candidate, v: Vacancy, relevance: int, reasons: List[str]) -> str:
    base = [
        f"–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {relevance}%.",
        f"–ö–∞–Ω–¥–∏–¥–∞—Ç: {c.full_name or '‚Äî'}, –ø–æ–∑–∏—Ü–∏—è: {c.position or '‚Äî'}, –≥–æ—Ä–æ–¥: {c.city or '‚Äî'}.",
        f"–û–ø—ã—Ç: {c.experience_years if c.experience_years is not None else '‚Äî'} –ª–µ—Ç; –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ: {c.education or '‚Äî'}."
    ]
    base.append("–ü—Ä–∏—á–∏–Ω—ã –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–π: " + "; ".join(reasons) if reasons else "–°—É—â–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–π –Ω–µ –≤—ã—è–≤–ª–µ–Ω–æ.")
    return " ".join(base)

@app.post("/parse")
def parse_endpoint(req: ParseRequest):
    if req.kind not in ("cv","vacancy"):
        raise HTTPException(400, "kind must be 'cv' or 'vacancy'")
    return {"candidate": extract_candidate(req.text).dict()} if req.kind=="cv" else {"vacancy": extract_vacancy(req.text).dict()}

@app.post("/analyze", response_model=AnalyzeResult)
def analyze(req: AnalyzeRequest):
    candidate = req.cv or (extract_candidate(req.cv_text) if req.cv_text else None)
    vacancy   = req.vacancy or (extract_vacancy(req.vacancy_text) if req.vacancy_text else None)
    if not candidate or not vacancy:
        raise HTTPException(400, "Provide (cv or cv_text) AND (vacancy or vacancy_text)")
    relevance, mismatches, reasons = score_and_mismatches(candidate, vacancy)
    questions = make_followups(mismatches)
    summary = employer_summary(candidate, vacancy, relevance, reasons)

    session_id = req.session_id or str(uuid.uuid4())
    SESSIONS.setdefault(session_id, {"created_at": datetime.utcnow().isoformat(), "candidate": candidate.dict(), "vacancy": vacancy.dict(), "chat":[]})
    SESSIONS[session_id]["last_analysis"] = {"relevance":relevance,"mismatches":mismatches,"reasons":reasons,"summary":summary,"questions":questions}

    return AnalyzeResult(
        session_id=session_id,
        relevance_percent=relevance,
        reasons=reasons,
        mismatches=mismatches,
        followup_questions=questions,
        candidate=candidate,
        vacancy=vacancy,
        summary_for_employer=summary
    )

@app.post("/chat/turn", response_model=ChatResult)
def chat_turn(turn: ChatTurn):
    sess = SESSIONS.get(turn.session_id)
    if not sess: raise HTTPException(404, "session_id not found")
    sess["chat"].append({"role":"candidate","text":turn.message_from_candidate,"ts":datetime.utcnow().isoformat()})
    c = Candidate(**sess["candidate"]); v = Vacancy(**sess["vacancy"])
    relevance, mismatches, reasons = score_and_mismatches(c, v)
    questions = make_followups(mismatches)
    summary = employer_summary(c, v, relevance, reasons)
    bot_replies = questions[:2] if questions else ["–°–ø–∞—Å–∏–±–æ! –£—Ç–æ—á–Ω–µ–Ω–∏–π –±–æ–ª—å—à–µ –Ω–µ—Ç."]
    for q in bot_replies:
        sess["chat"].append({"role":"bot","text":q,"ts":datetime.utcnow().isoformat()})
    sess["last_analysis"] = {"relevance":relevance,"mismatches":mismatches,"reasons":reasons,"summary":summary,"questions":questions}
    return ChatResult(session_id=turn.session_id, bot_replies=bot_replies, relevance_percent=relevance, reasons=reasons, summary_for_employer=summary)

@app.get("/sessions/{session_id}")
def get_session(session_id: str):
    sess = SESSIONS.get(session_id)
    if not sess: raise HTTPException(404, "session not found")
    return sess

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8001)
